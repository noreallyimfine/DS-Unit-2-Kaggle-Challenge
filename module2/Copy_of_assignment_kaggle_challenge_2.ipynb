{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment_kaggle_challenge_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noreallyimfine/DS-Unit-2-Kaggle-Challenge/blob/master/module2/Copy_of_assignment_kaggle_challenge_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXUfiQ2UKj6",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Kaggle Challenge, Module 2\n",
        "\n",
        "## Assignment\n",
        "- [x] Read [“Adopting a Hypothesis-Driven Workflow”](https://outline.com/5S5tsB), a blog post by a Lambda DS student about the Tanzania Waterpumps challenge.\n",
        "- [x] Continue to participate in our Kaggle challenge.\n",
        "- [x] Try Ordinal Encoding.\n",
        "- [x] Try a Random Forest Classifier.\n",
        "- [x] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
        "- [x] Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "## Stretch Goals\n",
        "\n",
        "### Doing\n",
        "- [ ] Add your own stretch goal(s) !\n",
        "- [ ] Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n",
        "- [ ] Try other [categorical encodings](https://contrib.scikit-learn.org/categorical-encoding/).\n",
        "- [ ] Get and plot your feature importances.\n",
        "- [ ] Make visualizations and share on Slack.\n",
        "\n",
        "### Reading\n",
        "\n",
        "Top recommendations in _**bold italic:**_\n",
        "\n",
        "#### Decision Trees\n",
        "- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and _**[Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)**_\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
        "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n",
        "\n",
        "#### Random Forests\n",
        "- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 8: Tree-Based Methods\n",
        "- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n",
        "- _**[Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)**_\n",
        "\n",
        "#### Categorical encoding for trees\n",
        "- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
        "- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n",
        "- _**[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)**_\n",
        "- _**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)**_\n",
        "- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n",
        "- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)\n",
        "\n",
        "#### Imposter Syndrome\n",
        "- [Effort Shock and Reward Shock (How The Karate Kid Ruined The Modern World)](http://www.tempobook.com/2014/07/09/effort-shock-and-reward-shock/)\n",
        "- [How to manage impostor syndrome in data science](https://towardsdatascience.com/how-to-manage-impostor-syndrome-in-data-science-ad814809f068)\n",
        "- [\"I am not a real data scientist\"](https://brohrer.github.io/imposter_syndrome.html)\n",
        "- _**[Imposter Syndrome in Data Science](https://caitlinhudon.com/2018/01/19/imposter-syndrome-in-data-science/)**_\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9eSnDYhUGD7",
        "colab_type": "code",
        "outputId": "bab22ffa-95b5-4732-ab26-7e06e6a777f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# If you're in Colab...\n",
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    # Install required python packages:\n",
        "    # category_encoders, version >= 2.0\n",
        "    # pandas-profiling, version >= 2.0\n",
        "    # plotly, version >= 4.0\n",
        "    !pip install --upgrade category_encoders pandas-profiling plotly\n",
        "    \n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module2')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/a1/f7a22f144f33be78afeb06bfa78478e8284a64263a3c09b1ef54e673841e/category_encoders-2.0.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
            "\u001b[?25hCollecting pandas-profiling\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/2f/aae19e2173c10a9bb7fee5f5cad35dbe53a393960fc91abc477dcc4661e8/pandas-profiling-2.3.0.tar.gz (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.8MB/s \n",
            "\u001b[?25hCollecting plotly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/2b/4ca10995bfbdefd65c4238f9a2d3fde33705d18dd50914dd13302ec1daf1/plotly-4.1.0-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.4 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: jinja2>=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (2.10.1)\n",
            "Requirement already satisfied, skipping upgrade: missingno>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (0.4.2)\n",
            "Collecting htmlmin>=0.1.12 (from pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
            "Collecting phik>=0.9.8 (from pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/ad/24a16fa4ba612fb96a3c4bb115a5b9741483f53b66d3d3afd987f20fa227/phik-0.9.8-py3-none-any.whl (606kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 39.6MB/s \n",
            "\u001b[?25hCollecting confuse>=1.0.0 (from pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/6f/90e860cba937c174d8b3775729ccc6377eb91f52ad4eeb008e7252a3646d/confuse-1.0.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: astropy in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.6/dist-packages (from missingno>=0.4.2->pandas-profiling) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: numba>=0.38.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (0.40.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=5.2.3 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.3.1)\n",
            "Collecting pytest-pylint>=0.13.0 (from phik>=0.9.8->pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/64/dc/6f35f114844fb12e38d60c4f3d2441a55baff7043ad4e013777dff55746c/pytest_pylint-0.14.1-py3-none-any.whl\n",
            "Collecting pytest>=4.0.2 (from phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1d/2430053122a3c6106f7fd1ff0bc68eb73e27db8f951db70fcd942da52c7b/pytest-5.0.1-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nbconvert>=5.3.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from confuse>=1.0.0->pandas-profiling) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling) (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.5.3)\n",
            "Requirement already satisfied, skipping upgrade: traitlets in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (17.0.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.5.0)\n",
            "Collecting pylint>=1.4.5 (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl (765kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (7.2.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.19)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1)\n",
            "Collecting pluggy<1.0,>=0.12 (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/06/ee/de89e0582276e3551df3110088bf20844de2b0e7df2748406876cc78e021/pluggy-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets->jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (0.2.0)\n",
            "Collecting mccabe<0.7,>=0.6 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting astroid<3,>=2.2.0 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.8MB/s \n",
            "\u001b[?25hCollecting isort<5,>=4.2.5 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12->pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.11.2)\n",
            "Collecting lazy-object-proxy (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/2a/d73b99e9407be3acd7c0328fcc44bcf6f5c42e6d03d1fb192032c0057d13/lazy_object_proxy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.2MB/s \n",
            "\u001b[?25hCollecting typed-ast>=1.3.0; implementation_name == \"cpython\" (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 39.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pandas-profiling, htmlmin, confuse\n",
            "  Building wheel for pandas-profiling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-profiling: filename=pandas_profiling-2.3.0-py2.py3-none-any.whl size=145036 sha256=cffa62ed3709c42673d71662b931f9d6e7864f64d10a3b41da1bc362be5c914c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/c7/f1/dbfef4848ebb048cb1d4a22d1ed0c62d8ff2523747235e19fe\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp36-none-any.whl size=27085 sha256=cf4aa163d8ee5290c8241e76363e2d0bdc9c161bead75d5256c034f610c3c7db\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
            "  Building wheel for confuse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for confuse: filename=confuse-1.0.0-cp36-none-any.whl size=17487 sha256=5ceb8dda60b806542f6dc9c533472a501ef2208d75ace6065fe1127089f62121\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/b2/96/2074eee7dbf7b7df69d004c9b6ac4e32dad04fb7666cf943bd\n",
            "Successfully built pandas-profiling htmlmin confuse\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: category-encoders, htmlmin, pluggy, pytest, mccabe, lazy-object-proxy, typed-ast, astroid, isort, pylint, pytest-pylint, phik, confuse, pandas-profiling, plotly\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "  Found existing installation: plotly 3.6.1\n",
            "    Uninstalling plotly-3.6.1:\n",
            "      Successfully uninstalled plotly-3.6.1\n",
            "Successfully installed astroid-2.2.5 category-encoders-2.0.0 confuse-1.0.0 htmlmin-0.1.12 isort-4.3.21 lazy-object-proxy-1.4.1 mccabe-0.6.1 pandas-profiling-2.3.0 phik-0.9.8 plotly-4.1.0 pluggy-0.12.0 pylint-2.3.1 pytest-5.0.1 pytest-pylint-0.14.1 typed-ast-1.4.0\n",
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 4 (delta 0), pack-reused 28\n",
            "Unpacking objects: 100% (33/33), done.\n",
            "From https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge\n",
            " * branch            master     -> FETCH_HEAD\n",
            " * [new branch]      master     -> origin/master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJBD4ruICm1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
        "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
        "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4yOXlx2Z5Dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee4070f0-e46c-4144-d373-9345e7f9362b"
      },
      "source": [
        "train, val = train_test_split(train, train_size=0.8, test_size=0.2,\n",
        "                               stratify=train['status_group'], random_state=42)\n",
        "\n",
        "train.shape, val.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47520, 41), (11880, 41))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNYyplfwY_2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fumction to wrangle all the data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def wrangle(X):\n",
        "\n",
        "  # Prevent SettingWithCopyWarning\n",
        "  X = X.copy()\n",
        "\n",
        "  # Treat zeros as nulls, convert near zeros to zero\n",
        "  X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "\n",
        "  # Replace suspicious zeroes with nans, then create new\n",
        "  # column with binary variable for whether it was missing\n",
        "  # will impute values later\n",
        "  cols_with_zeros = ['longitude', 'latitude', 'construction_year',\n",
        "                     'gps_height', 'population']\n",
        "  for col in cols_with_zeros:\n",
        "    X[col] = X[col].replace(0, np.nan)\n",
        "    X[col+'_MISSING'] = X[col].isnull()\n",
        "\n",
        "  # Convert date_recorded to datetime object\n",
        "  X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True) \n",
        "\n",
        "  # Extract year, month, and day from date_recorded and drop date_recorded\n",
        "  X['year_recorded'] = X['date_recorded'].dt.year\n",
        "  X['month_recorded'] = X['date_recorded'].dt.month\n",
        "  X['day_recorded'] = X['date_recorded'].dt.day\n",
        "  X = X.drop(columns='date_recorded') \n",
        "\n",
        "  # New column time_from_construction_to_inspection\n",
        "  X['years_til_inspection'] = X['year_recorded'] - X['construction_year'] \n",
        "  X['years_missing'] = X['years_til_inspection'].isnull()\n",
        "  \n",
        "\n",
        "  # quantity group is a duplicate of quantity so drop\n",
        "  X = X.drop(columns=['quantity_group', 'scheme_management', 'num_private', \n",
        "                      'recorded_by', 'payment', 'waterpoint_type'])\n",
        "\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv-KCCzWZ3rH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrangle data \n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNO4xtUaWI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94afb0ae-5b99-4dce-c744-5921307ebaa1"
      },
      "source": [
        "train.shape, val.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47520, 44), (11880, 44))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miu22sCXcGDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "339b60b5-2890-4533-c278-f9c132e504a5"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                    int64\n",
              "amount_tsh                          float64\n",
              "date_recorded                datetime64[ns]\n",
              "funder                               object\n",
              "gps_height                          float64\n",
              "installer                            object\n",
              "longitude                           float64\n",
              "latitude                            float64\n",
              "wpt_name                             object\n",
              "basin                                object\n",
              "subvillage                           object\n",
              "region                               object\n",
              "region_code                           int64\n",
              "district_code                         int64\n",
              "lga                                  object\n",
              "ward                                 object\n",
              "population                          float64\n",
              "public_meeting                       object\n",
              "scheme_name                          object\n",
              "permit                               object\n",
              "construction_year                   float64\n",
              "extraction_type                      object\n",
              "extraction_type_group                object\n",
              "extraction_type_class                object\n",
              "management                           object\n",
              "management_group                     object\n",
              "payment_type                         object\n",
              "water_quality                        object\n",
              "quality_group                        object\n",
              "quantity                             object\n",
              "source                               object\n",
              "source_type                          object\n",
              "source_class                         object\n",
              "waterpoint_type_group                object\n",
              "status_group                         object\n",
              "longitude_MISSING                      bool\n",
              "latitude_MISSING                       bool\n",
              "construction_year_MISSING              bool\n",
              "gps_height_MISSING                     bool\n",
              "population_MISSING                     bool\n",
              "years_til_inspection                float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fhUAAOvbTAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select all features but date_recorded and the target\n",
        "features = train.columns.drop(['status_group', 'date_recorded']).tolist()\n",
        "target = 'status_group'\n",
        "\n",
        "# Create X matrix and y vector\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5hiReLHaXaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import category_encoders as ce"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOeOYWnfa0BH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5154bd58-c321-4ed2-c96c-46481504a63c"
      },
      "source": [
        "# Create pipeline, Ordinal Encoder, RandomForest depth of 12\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=12, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "print('Training Score: ', pipeline.score(X_train, y_train))\n",
        "print('Validation Score: ', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Score:  0.8247053872053872\n",
            "Validation Score:  0.7856060606060606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jPR0jyb7Af",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "3bf65fb5-14b5-4237-d185-c47a20c6d1f8"
      },
      "source": [
        "# Loop through random forest depths 10-19\n",
        "for depth in range(10, 20):\n",
        "  pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42, n_jobs=-1)\n",
        "  )\n",
        "\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  print('Forest with Trees of ', depth, 'depth')\n",
        "  print('Training Score: ', pipeline.score(X_train, y_train))\n",
        "  print('Validation Score: ', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forest with Trees of  10 depth\n",
            "Training Score:  0.7846380471380472\n",
            "Validation Score:  0.7633838383838384\n",
            "Forest with Trees of  11 depth\n",
            "Training Score:  0.8059132996632996\n",
            "Validation Score:  0.7765151515151515\n",
            "Forest with Trees of  12 depth\n",
            "Training Score:  0.8247053872053872\n",
            "Validation Score:  0.7856060606060606\n",
            "Forest with Trees of  13 depth\n",
            "Training Score:  0.8461069023569023\n",
            "Validation Score:  0.7901515151515152\n",
            "Forest with Trees of  14 depth\n",
            "Training Score:  0.8645412457912458\n",
            "Validation Score:  0.7977272727272727\n",
            "Forest with Trees of  15 depth\n",
            "Training Score:  0.8862163299663299\n",
            "Validation Score:  0.7984848484848485\n",
            "Forest with Trees of  16 depth\n",
            "Training Score:  0.9064604377104377\n",
            "Validation Score:  0.8047138047138047\n",
            "Forest with Trees of  17 depth\n",
            "Training Score:  0.9249158249158249\n",
            "Validation Score:  0.805050505050505\n",
            "Forest with Trees of  18 depth\n",
            "Training Score:  0.9432870370370371\n",
            "Validation Score:  0.8055555555555556\n",
            "Forest with Trees of  19 depth\n",
            "Training Score:  0.9578914141414141\n",
            "Validation Score:  0.8066498316498316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgvK3np3qouq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5453af36-7d08-4cf3-9713-56450fa9a5b2"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'amount_tsh', 'funder', 'gps_height', 'installer', 'longitude',\n",
              "       'latitude', 'wpt_name', 'basin', 'subvillage', 'region', 'region_code',\n",
              "       'district_code', 'lga', 'ward', 'population', 'public_meeting',\n",
              "       'scheme_name', 'permit', 'construction_year', 'extraction_type',\n",
              "       'extraction_type_group', 'extraction_type_class', 'management',\n",
              "       'management_group', 'payment_type', 'water_quality', 'quality_group',\n",
              "       'quantity', 'source', 'source_type', 'source_class',\n",
              "       'waterpoint_type_group', 'status_group', 'longitude_MISSING',\n",
              "       'latitude_MISSING', 'construction_year_MISSING', 'gps_height_MISSING',\n",
              "       'population_MISSING', 'year_recorded', 'month_recorded', 'day_recorded',\n",
              "       'years_til_inspection', 'years_missing'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIjqTA-xdmnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another all features model, after updating wrangle function\n",
        "features = train.columns.drop('status_group').tolist()\n",
        "target = 'status_group'\n",
        "\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlntNQfXp8on",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "8a4dd0c8-f5a9-4d7d-e2fd-85dea55169c3"
      },
      "source": [
        "# Again testing different max depths\n",
        "for depth in range(10, 20):\n",
        "  pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy='median'),\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42, n_jobs=-1)\n",
        "  )\n",
        "\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  print('Forest with Trees of ', depth, 'depth')\n",
        "  print('Training Score: ', pipeline.score(X_train, y_train))\n",
        "  print('Validation Score: ', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forest with Trees of  10 depth\n",
            "Training Score:  0.7853324915824916\n",
            "Validation Score:  0.7653198653198653\n",
            "Forest with Trees of  11 depth\n",
            "Training Score:  0.8046506734006734\n",
            "Validation Score:  0.7774410774410775\n",
            "Forest with Trees of  12 depth\n",
            "Training Score:  0.8247474747474748\n",
            "Validation Score:  0.7853535353535354\n",
            "Forest with Trees of  13 depth\n",
            "Training Score:  0.8456860269360269\n",
            "Validation Score:  0.7932659932659932\n",
            "Forest with Trees of  14 depth\n",
            "Training Score:  0.866729797979798\n",
            "Validation Score:  0.7982323232323232\n",
            "Forest with Trees of  15 depth\n",
            "Training Score:  0.8863005050505051\n",
            "Validation Score:  0.8012626262626262\n",
            "Forest with Trees of  16 depth\n",
            "Training Score:  0.9056607744107744\n",
            "Validation Score:  0.8031986531986532\n",
            "Forest with Trees of  17 depth\n",
            "Training Score:  0.9259048821548822\n",
            "Validation Score:  0.8071548821548822\n",
            "Forest with Trees of  18 depth\n",
            "Training Score:  0.942929292929293\n",
            "Validation Score:  0.8083333333333333\n",
            "Forest with Trees of  19 depth\n",
            "Training Score:  0.9570707070707071\n",
            "Validation Score:  0.8101851851851852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpQf2pn_sYIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a8230357-a511-40d5-fc4a-3ffcb53f7547"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'amount_tsh', 'funder', 'gps_height', 'installer', 'longitude',\n",
              "       'latitude', 'wpt_name', 'basin', 'subvillage', 'region', 'region_code',\n",
              "       'district_code', 'lga', 'ward', 'population', 'public_meeting',\n",
              "       'scheme_name', 'permit', 'construction_year', 'extraction_type',\n",
              "       'extraction_type_group', 'extraction_type_class', 'management',\n",
              "       'management_group', 'payment_type', 'water_quality', 'quality_group',\n",
              "       'quantity', 'source', 'source_type', 'source_class',\n",
              "       'waterpoint_type_group', 'status_group', 'longitude_MISSING',\n",
              "       'latitude_MISSING', 'construction_year_MISSING', 'gps_height_MISSING',\n",
              "       'population_MISSING', 'year_recorded', 'month_recorded', 'day_recorded',\n",
              "       'years_til_inspection', 'years_missing'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZqNxcJuqB9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select features\n",
        "# Iterated through for meaningful features\n",
        "categorical_features = ['region', 'quantity', 'quality_group',\n",
        "                        'waterpoint_type_group', 'source_type',\n",
        "                        'extraction_type_class', 'water_quality',\n",
        "                        'extraction_type', 'gps_height_MISSING']\n",
        "numerical_features = ['longitude', 'latitude', 'construction_year',\n",
        "                      'years_til_inspection', 'gps_height', 'amount_tsh',\n",
        "                      'district_code', 'year_recorded', 'day_recorded']\n",
        "\n",
        "features = categorical_features + numerical_features\n",
        "target = 'status_group'\n",
        "\n",
        "# X matrix and y vector\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sskTd2N2r-DP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ce163af8-2a78-4207-e0d5-42c942cd3c4b"
      },
      "source": [
        "# Another pipeline, tree depth of 14, imputing median\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy='median'),\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=14, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "#print('Forest with Trees of ', depth, 'depth')\n",
        "print('Training Score: ', pipeline.score(X_train, y_train))\n",
        "print('Validation Score: ', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Score:  0.8494739057239057\n",
            "Validation Score:  0.7975589225589226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Wq5nw-txJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4UUpoorsILV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "820e147a-f913-4157-f68f-c478a92ff03a"
      },
      "source": [
        "# Submission\n",
        "submission = sample_submission.copy()\n",
        "submission['status_group'] = y_pred\n",
        "submission.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14358, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0sUbCKkt39u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save submissions\n",
        "submission.to_csv('submission5.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meCvnxcDt9KV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}